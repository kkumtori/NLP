{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1nrLOKFklhkai8s-GjApod-4pDj8GHykd","authorship_tag":"ABX9TyM98pkusUL8x5ozcFJqzFBk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 2. Text Preprocessing  \n","+ Tokenization : 토큰화\n","+ Cleaning : 정제 \n","+ Normalization : 정규화"],"metadata":{"id":"kEk_AXJ1eXMu"}},{"cell_type":"markdown","source":["## 2-1. 토큰화(Tokenization)  \n","`Corpus(말뭉치)` -> `Token(토큰)`  \n","+ 토큰 : 의미있는 단위"],"metadata":{"id":"l6790HYKeoaB"}},{"cell_type":"markdown","source":["### 2-1-1. 단어 토큰화(Word Tokenization)  \n","토큰의 기준을 단어로 하면 `word tokenization`이라고 함  \n","+ `단어` : 단어/단어구/유의미한 문자열 통칭함   \n","+ 예시 : 구두점 제외 토큰화  \n","    + 구두점 Cleaning\n","    + 띄어쓰기 기준으로 자르기\n","\n","__NLTK__는 영어 코퍼스를 토큰화하기 위한 도구들을 제공  \n","+ word_tokenize\n","+ WordPunctTokenizer"],"metadata":{"id":"opGeSnH-fcC0"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kRzFiV9mitdD","executionInfo":{"status":"ok","timestamp":1672486055317,"user_tz":-540,"elapsed":1134,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"70f9e159-a4a1-482f-d2fb-566f6a3b3bfc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","from nltk.tokenize import WordPunctTokenizer\n","from tensorflow.keras.preprocessing.text import text_to_word_sequence"],"metadata":{"id":"eaDkNX5TgPTS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# word_tokenize\n","string=\"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"\n","print('Word Tokenize v1 : ',word_tokenize(string))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"USNhftlShfXt","executionInfo":{"status":"ok","timestamp":1672486056467,"user_tz":-540,"elapsed":11,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"5209dd9f-d507-43cc-ae42-ddfa5c00e980"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Word Tokenize v1 :  ['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"]}]},{"cell_type":"code","source":["# WordPunctTokenizer\n","print('Word Tokenize v2 : ',\n","      WordPunctTokenizer().tokenize(string))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0k-2eOwwigXV","executionInfo":{"status":"ok","timestamp":1672486056468,"user_tz":-540,"elapsed":10,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"e70ce3e0-1205-4532-828c-14ded6a3d6b6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Word Tokenize v2 :  ['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"]}]},{"cell_type":"markdown","source":["WordPunctTokenizer는 구두점을 별도로 분류함"],"metadata":{"id":"oSOIWwljjReg"}},{"cell_type":"code","source":["# keras text_to_word_sequence\n","print('Word Tokenize v3 : ',\n","      text_to_word_sequence(string))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lY-TCSRjjnw6","executionInfo":{"status":"ok","timestamp":1672486056468,"user_tz":-540,"elapsed":7,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"d12b8c26-37ab-4d5e-9c55-19c57c9ad7ae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Word Tokenize v3 :  [\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"]}]},{"cell_type":"markdown","source":["keras.text_to_word_sequence는 모두 소문자 변환, 구두점제거, `'`는 제거 안함"],"metadata":{"id":"WE9BApUNj7N9"}},{"cell_type":"markdown","source":["### 2-1-3. 토큰화에서 고려해야할 사항  \n","+ 구두점/특수문자 단순 제외 처리 안됨  \n","    ex) Ph.D, $45.55 등등..\n","+ 줄임말과 단어 내 띄어쓰기\n","    ex) we're : we are (clitic 접어), New York  \n","+ 표준 토큰화 예제  \n","__Penn Treeback Tokenization__ :표준 토큰화 방법\n","    + Rule1) -하이픈으로 구성된 단어는 하나로 유지\n","    + Rule2) doesn't와 같이 <'>로 접어가 함께하는 단어는 분리한다."],"metadata":{"id":"R6vMzNUGkHqG"}},{"cell_type":"code","source":["from nltk.tokenize import TreebankWordTokenizer\n","\n","tokenizer=TreebankWordTokenizer()\n","\n","text=\"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n","print('Treebank WT : ',\n","      tokenizer.tokenize(text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0z4wuhTsnI_m","executionInfo":{"status":"ok","timestamp":1672486056468,"user_tz":-540,"elapsed":5,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"b18a2d5f-70d6-4b19-bbb3-49ae26632bac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Treebank WT :  ['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"]}]},{"cell_type":"markdown","source":["### 2-1-4. 문장 토큰화 (Sentence Tokenization)  \n","토큰의 단위가 문장일 때,  \n","코퍼스 -> 문장 단위로 구분  \n","__Sentence Segmentation__ \n","+ 구두점으로 구분하자! -> 안됨 <.>의 경우 예외 존나 많다"],"metadata":{"id":"G303s7cXnrcO"}},{"cell_type":"code","source":["# nltk.tokenize.sent_tokenize\n","from nltk.tokenize import sent_tokenize\n","\n","text=\"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n","print('Sentence Toknization v1 : ',\n","      sent_tokenize(text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WZZaZgQ8pJvL","executionInfo":{"status":"ok","timestamp":1672486058893,"user_tz":-540,"elapsed":6,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"4b6ab822-cdd6-4471-ccae-0933cd3f0353"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentence Toknization v1 :  ['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"]}]},{"cell_type":"markdown","source":["Good!"],"metadata":{"id":"W3gdjCskqAMk"}},{"cell_type":"code","source":["text= \"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n","print('Sentence Toknization v2 : ',\n","      sent_tokenize(text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P9b_yuJRqHUl","executionInfo":{"status":"ok","timestamp":1672486060031,"user_tz":-540,"elapsed":2,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"07ee04a9-b3ba-42f7-edf9-c2bcb385107e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentence Toknization v2 :  ['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"]}]},{"cell_type":"markdown","source":["Good!  \n","Ph.D 잘 넘어감!"],"metadata":{"id":"_ObDkbhAqP24"}},{"cell_type":"code","source":["# KSS(Korean Sentence Splitter)\n","!pip install kss"],"metadata":{"id":"cVShsqa3qqqv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import kss\n","\n","text='딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?'\n","print('KSS : ',\n","      kss.split_sentences(text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CihhZvK6qYmD","executionInfo":{"status":"ok","timestamp":1672486061171,"user_tz":-540,"elapsed":2,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"64fb3c8b-75e6-4ffe-9abe-ca940ee7eff2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["KSS :  ['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다.', '이제 해보면 알걸요?']\n"]}]},{"cell_type":"markdown","source":["### 2-1-5. 한국어에서의 토큰화의 어려움  \n","+ 교착어의 특성  \n","    + `교착어` : 조사, 어미 등을 붙여서 말을 만드는 언어  \n","    + `형태소`(morpheme)  \n","        + __자립형태소__: 그 자체로 단어가 되는 형태소\n","            체언(명사, 대명사, 수사), 수식언(관형사, 부사), 감탄사 등   \n","        + __의존형태소__: 다른 형태소와 결합하여 사용되는 형태소  \n","            접사,어미,조사,어간  \n","        ex) _에디가 책을 읽었다_  \n","            자립 : 에디, 책  \n","            의존 : -가, -을, 읽-,-었-,-다  \n","        + 한국어에서는 `형태소 토큰화`를 하자!  \n","+ 띄어쓰기가 애매함 -> 굳이 안해도 해석될 때가 많음"],"metadata":{"id":"ztCKHuo2qpeH"}},{"cell_type":"markdown","source":["### 2-1-6. 품사 태깅 (Part-of-speech Taggging)  \n","ex. __못__ \n","+ 명사 : ⚒️\n","+ 부사 : ❌ "],"metadata":{"id":"CSf3TPpq1_MW"}},{"cell_type":"markdown","source":["### 2-1-7. NLTK와 KoNLPy를 이용한 영어, 한국어 토큰화 실습  \n","NLTK에서는 Pee Treebank POS Tags를 기준으로 품사 태깅함"],"metadata":{"id":"5BsiiY8N9OBg"}},{"cell_type":"code","source":["nltk.download('averaged_perceptron_tagger')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aw7RkOLP-bua","executionInfo":{"status":"ok","timestamp":1672486699938,"user_tz":-540,"elapsed":291,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"b99f2fb5-339d-4225-cb2d-f3535673590a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","from nltk.tag import pos_tag\n","\n","text=\"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n","tokenized_sentence=word_tokenize(text)\n","\n","print('Word Tokenization : ', tokenized_sentence)\n","print('Pos Tagging : ',pos_tag(tokenized_sentence))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AM7iXEff9pv5","executionInfo":{"status":"ok","timestamp":1672486701599,"user_tz":-540,"elapsed":309,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"819b195a-19af-4e6d-a5e9-d9681731d8d2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Word Tokenization :  ['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n","Pos Tagging :  [('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')]\n"]}]},{"cell_type":"markdown","source":["[참고]  \n","+ PRP : 인칭대명사\n","+ VBP : 동사\n","+ RB : 부사 ... 등등"],"metadata":{"id":"Tr1TVtz5-ALV"}},{"cell_type":"markdown","source":["KoNLPy 패키지에서 사용가능한 형태소 분석기 \n","+ Okt(Open Korea Text)\n","+ 메캅(Mecab)\n","+ 코모란(Komoran)\n","+ 한나눔(Hannanum)\n","+ 꼬꼬마(Kkma)"],"metadata":{"id":"SgfA6KU0_AKn"}},{"cell_type":"code","source":["!pip install konlpy"],"metadata":{"id":"nKg3FHwu_d6u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Morpheme Tokenization with KoNLPy\n","from konlpy.tag import Okt\n","from konlpy.tag import Kkma\n","\n","okt=Okt()\n","kkma=Kkma()\n","\n","text='열심히 코딩한 당신, 연휴에는 여행을 가봐요'\n","\n","print('Okt 형태소 분석: ',okt.morphs(text))\n","print('Okt 품사 태깅 : ',okt.pos(text))\n","print('Okt 명사 추출 : ',okt.nouns(text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cMonHMDj_Rvb","executionInfo":{"status":"ok","timestamp":1672487382670,"user_tz":-540,"elapsed":10591,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"13fd9b6e-f8cf-4221-cbd8-ca154b881c5d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Okt 형태소 분석:  ['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n","Okt 품사 태깅 :  [('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n","Okt 명사 추출 :  ['코딩', '당신', '연휴', '여행']\n"]}]},{"cell_type":"code","source":["print('Kkma 형태소 분석: ',kkma.morphs(text))\n","print('Kkma 품사 태깅 : ',kkma.pos(text))\n","print('Kkma 명사 추출 : ',kkma.nouns(text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"23f61_YD_cmj","executionInfo":{"status":"ok","timestamp":1672487819393,"user_tz":-540,"elapsed":17886,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"d547c002-0ed8-43ec-f1b4-b74ed914fbd6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Kkma 형태소 분석:  ['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n","Kkma 품사 태깅 :  [('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n","Kkma 명사 추출 :  ['코딩', '당신', '연휴', '여행']\n"]}]},{"cell_type":"markdown","source":["## 2-2. 정제(Cleaning)와 정규화(Normalization)  \n","[Recap] `Tokenization`  : Corpus ➡️ Token  \n","+ `Cleaning` : 노이즈 데이터 제거\n","+ `Normalization` : 표현 방법이 다른 단어들을 통하하여 같은 단어로 정규화\n"],"metadata":{"id":"qF_3rS-WbInh"}},{"cell_type":"markdown","source":["### 2-2-1. 규칙에 기반한 표기가 다른 단어들의 통합  \n","ex. US, USA 등\n","+ stemming : 어간 추출\n","+ lemmatization : 표제어 추출"],"metadata":{"id":"fV5e0AZAbIfh"}},{"cell_type":"markdown","source":["### 2-2-3. 불필요한 단어의 제거  \n","+ 불용어 제거\n","+ 등장 빈도가 적은 단어  \n","    ex. 100,000개의 메일 중 5번만 등장한 단어 -> spam/ham 구별하는 데 도움 안됨\n","+ 길이가 짧은 단어  \n","    영어는 길이가 2~3 이하인 단어를 제거하는 것만으로도 노이즈 제거 효과가 있음"],"metadata":{"id":"h-MQcqC8bIYt"}},{"cell_type":"code","source":["import re\n","text = \"I was wondering if anyone out there could enlighten me on this car.\"\n","\n","# 길이가 1~2인 단어들을 삭제\n","shortword=re.compile(r'\\W*\\b\\w{1,2}\\b')\n","shortword.sub('',text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"493AvLfneRtp","executionInfo":{"status":"ok","timestamp":1672495349712,"user_tz":-540,"elapsed":468,"user":{"displayName":"노지예","userId":"16611815830350888933"}},"outputId":"00979544-b649-4a44-f7f0-6169f56dbe5c"},"execution_count":71,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' was wondering anyone out there could enlighten this car.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":71}]},{"cell_type":"markdown","source":["## 2-3. 어간 추출(Stemming) & 표제어 추출(Lemmatization)  \n","Normalization 기법 중 코퍼스의 단어 개수를 줄이는 기법  \n","  \n","+ 서로 다른 단어지만 하나의 단어로 일반화시켜 문서 내 단어의 수를 줄이는 것이 목표  \n","+ 코퍼스의 복잡성을 줄이는 것이 정규화의 주목표"],"metadata":{"id":"kO0isyMBfrsu"}},{"cell_type":"markdown","source":["### 2-3-1. Lemmatization : 표제어 추출  \n","+ `표제어(Lemma)` : 사전형 단어  \n","> ex. am, are, is -> be(표제어)  \n"],"metadata":{"id":"PD9IToyBfsA7"}}]}